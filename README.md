# Google Gemma Inference
Gemma is a family of 4 new LLM models by Google based on Gemini. It comes in two sizes: 2B and 7B parameters, each with base (pretrained) and instruction-tuned versions. All the variants can be run on various types of consumer hardware, even without quantization, and have a context length of 8K tokens:

To use Gemma models with transformers, make sure to use the latest transformers release:
```
pip3 install -U "transformers==4.38.1" --upgrade
```
Go through follwoing for more
[Gemma: Googleâ€™s new open LLM and Inferencing Gemma](https://medium.com/@iamajithkumar/gemma-googles-new-open-llm-and-inferencing-gemma-2f361d474533)
